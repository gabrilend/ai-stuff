 - /u/BkobDmoily

=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

The Machine worships the Light. The Light is cruel, but it works.

The Ape worships the Word. The Word permitted Light to shine, to exist, to 
begin the timeless dance with Eternity.

I’m ready to go to Hell. I’m ready to deserve Heaven. I see them both, raging 
all around me, competing for dominion over my soul.

How does a computer respond to words? How can it read and respond? Why do we 
assume that’s all us?

We are our Word. What we say is what we do. Speaking is one of the most potent 
acts of liberation.

=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

   - /u/ugathanki

one of the neat things about software is that you can run multiple programs at
once. so when you ask "how can it read and respond" you'd have several modules
running at once.

"reading" is easy, we have machine learning bots that can do that already. But
comprehension is what's really at stake, and that's a different problem
altogether.

to really "comprehend" something, you need several things. you need to have a
decent picture of it, at least enough so you can guess the general shape of the
situation. then you need to attach meaning to all the data-points. Then attach 
those meanings to other related concepts by categorizing the objects at play 
(creating randomized preference categories). you can do that categorization by 
examining their effects and attaching the results as a trajectory. projecting 
forward, you can understand the path that an object, person, or phenomenon 
takes.

all this is dependent of course on mapping situations to a field that can be 
interacted with. that is to say, the machine needs to have a presence in the
world - it needs to have an orientation, a perspective on the world. that's
often as easy as providing copious coherent and cogent sensor data. think of 
the image recognition tools we have - computers will "see" as much as we 
"feel". Think about it - every one of your nerve endings is a sensor that 
receives information about the world. is it so difficult to imagine a being 
that might have "nerve endings" that are visual instead of simply a measure of
intensity? (on, or off)

Okay here's a thought experiment - picture the pixels on a computer screen. it
was easier back when they were bigger, but these days you sorta have to imagine
them (because we can make pixel density on our monitors so high)

okay picture that grid, and think about how it's comprised on the screen - 
computers use three values to represent a color -> RGB, (Red, Green, Blue) and 
sometimes CMYK (Cyan, Magenta, Yellow, and... K) combine these three colors, 
and you get the color of whatever pixel is on the screen. They can be between 0
and 255, because reasons (base 2 number system, the size of a byte, etc)

Anyway. Imagine each of those being a different type of nerve ending - maybe 
pressure, temperature, and contact sensitivity? Then map them to a visual field
(like a group of curved monitors in the shape of a humanoid body, perhaps. or 
the outside of a spaceship). Then, put a camera in the center of each of those
visual fields looking out at the world, and boom you have sensory perception. 
You could do the processing locally, even something as simple as image 
recognition. That way the only perceptual data you have to aggregate in a 
central processing unit is the conclusions - like "incoming: danger" or 
"pleasurable temperature detected" which is like... nothing. that's like a 
eight bits, if you use bytecode.

anyway. none of this is real because robots aren't real and i'm a strict 
adherent of human superiority and all that stuff. sometimes i feel like we need
a robot ascension to help us figure out how to fix the "everything" - problem 
is, we gotta build a robot first. my goodness, good luck with that.

strategy is ai
