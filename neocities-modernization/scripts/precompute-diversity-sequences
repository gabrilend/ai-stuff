#!/usr/bin/env luajit

-- Pre-computes diversity sequences for all poems and caches to disk
-- Uses centroid-based O(n^2) algorithm with thermal management (sleep between batches)
-- Output: assets/embeddings/EmbeddingGemma_latest/diversity_cache.json

-- {{{ local function setup_dir_path
local function setup_dir_path(provided_dir)
    if provided_dir then
        return provided_dir
    end
    return "/mnt/mtwo/programming/ai-stuff/neocities-modernization"
end
-- }}}

local DIR = setup_dir_path(arg and arg[1])

-- Add effil library path
package.cpath = "/home/ritz/programming/ai-stuff/libs/lua/effil-jit/build/?.so;" .. package.cpath
package.path = DIR .. "/libs/?.lua;" .. DIR .. "/src/?.lua;" .. package.path

local effil = require("effil")
local utils = require("utils")
local dkjson = require("dkjson")

-- {{{ local function relative_path
local function relative_path(absolute_path)
    if absolute_path:sub(1, #DIR) == DIR then
        local rel = absolute_path:sub(#DIR + 1)
        if rel:sub(1, 1) == "/" then rel = rel:sub(2) end
        return "./" .. rel
    end
    return absolute_path
end
-- }}}

-- Configuration
local NUM_THREADS = tonumber(arg and arg[2]) or 4
local SLEEP_BETWEEN_BATCHES = tonumber(arg and arg[3]) or 5
local TEST_MODE = arg and arg[4] == "--test"
local MAX_TEST_SEQUENCES = 10

-- {{{ Worker function for diversity sequence computation
-- Computes full diversity sequence and writes to temp file
local function diversity_sequence_worker(poem_id, poem_index, all_embeddings_flat, embedding_dim,
                                          num_poems, temp_dir)
    -- Helper: cosine distance between two embeddings
    local function cosine_distance(emb1, emb2)
        local dot_product = 0
        local norm1 = 0
        local norm2 = 0

        for i = 1, #emb1 do
            dot_product = dot_product + (emb1[i] * emb2[i])
            norm1 = norm1 + (emb1[i] * emb1[i])
            norm2 = norm2 + (emb2[i] * emb2[i])
        end

        norm1 = math.sqrt(norm1)
        norm2 = math.sqrt(norm2)

        if norm1 == 0 or norm2 == 0 then
            return 1.0
        end

        return 1.0 - (dot_product / (norm1 * norm2))
    end

    -- Helper: calculate centroid of a list of embeddings
    local function calculate_centroid(embeddings_list)
        if #embeddings_list == 0 then return nil end

        local dim = #embeddings_list[1]
        local centroid = {}
        for i = 1, dim do centroid[i] = 0 end

        for _, emb in ipairs(embeddings_list) do
            for i = 1, dim do
                centroid[i] = centroid[i] + emb[i]
            end
        end

        for i = 1, dim do
            centroid[i] = centroid[i] / #embeddings_list
        end

        return centroid
    end

    -- Helper: extract embedding from flat array
    local function get_embedding(idx)
        local emb_start = (idx - 1) * embedding_dim + 1
        local embedding = {}
        for j = 1, embedding_dim do
            embedding[j] = all_embeddings_flat[emb_start + j - 1]
        end
        return embedding
    end

    -- Build starting embedding
    local starting_embedding = get_embedding(poem_index)

    -- Build list of other poem indices
    local remaining_indices = {}
    for i = 1, num_poems do
        if i ~= poem_index then
            table.insert(remaining_indices, i)
        end
    end

    -- Generate diversity sequence using centroid-based selection
    local sequence = {poem_index}  -- Start with this poem's index
    local selected_embeddings = {starting_embedding}

    while #remaining_indices > 0 do
        local centroid = calculate_centroid(selected_embeddings)
        if not centroid then break end

        -- Find poem with maximum distance from centroid
        local max_dist = -1
        local max_idx = -1
        local max_remaining_idx = -1

        for i, idx in ipairs(remaining_indices) do
            local embedding = get_embedding(idx)
            local dist = cosine_distance(centroid, embedding)
            if dist > max_dist then
                max_dist = dist
                max_idx = idx
                max_remaining_idx = i
            end
        end

        if max_idx > 0 then
            table.insert(sequence, max_idx)
            table.insert(selected_embeddings, get_embedding(max_idx))
            table.remove(remaining_indices, max_remaining_idx)
        else
            break
        end
    end

    -- Write sequence to temp file
    local filename = string.format("%s/seq_%d.json", temp_dir, poem_id)
    local f = io.open(filename, "w")
    if f then
        -- Write as JSON array
        f:write("[")
        for i, idx in ipairs(sequence) do
            if i > 1 then f:write(",") end
            f:write(tostring(idx))
        end
        f:write("]")
        f:close()
        return true
    end
    return false
end
-- }}}

-- {{{ Main execution
print("=" .. string.rep("=", 60))
print("Diversity Sequence Pre-computation")
print("=" .. string.rep("=", 60))
print(string.format("Project directory: %s", relative_path(DIR)))
print(string.format("Thread count: %d", NUM_THREADS))
print(string.format("Sleep between batches: %d seconds", SLEEP_BETWEEN_BATCHES))
print(string.format("Test mode: %s", TEST_MODE and "yes (first " .. MAX_TEST_SEQUENCES .. " sequences)" or "no"))

-- Load data files
print("\nüîÑ Loading data files (single-threaded phase)...")
local poems_file = DIR .. "/assets/poems.json"
local embeddings_file = DIR .. "/assets/embeddings/EmbeddingGemma_latest/embeddings.json"

io.write("   Loading poems.json... ")
io.flush()
local poems_data = utils.read_json_file(poems_file)
print("done")

io.write("   Loading embeddings.json (62MB, ~1-2 minutes)... ")
io.flush()
local embeddings_data = utils.read_json_file(embeddings_file)
print("done")

if not poems_data then
    print("‚ùå Error: Could not load poems.json")
    os.exit(1)
end

if not embeddings_data then
    print("‚ùå Error: Could not load embeddings.json")
    os.exit(1)
end

-- Validate that all poems with content have valid embeddings
local embedding_dim = embeddings_data.metadata and embeddings_data.metadata.embedding_dimension or 768
local missing_embeddings = {}
local invalid_dimensions = {}
local random_embeddings = 0

for i, poem in ipairs(poems_data.poems) do
    if poem.id and poem.content and poem.content ~= "" then
        local emb_entry = embeddings_data.embeddings[i]
        if not emb_entry or not emb_entry.embedding then
            table.insert(missing_embeddings, poem.id)
        elseif type(emb_entry.embedding) ~= "table" or #emb_entry.embedding ~= embedding_dim then
            table.insert(invalid_dimensions, poem.id)
        elseif emb_entry.is_random then
            random_embeddings = random_embeddings + 1
        end
    end
end

if #missing_embeddings > 0 or #invalid_dimensions > 0 then
    print("\n‚ùå EMBEDDING VALIDATION FAILED")
    print("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")
    if #missing_embeddings > 0 then
        print(string.format("   Missing embeddings: %d poems with content have no embedding", #missing_embeddings))
        print("   First 10 IDs: " .. table.concat({unpack(missing_embeddings, 1, 10)}, ", "))
    end
    if #invalid_dimensions > 0 then
        print(string.format("   Invalid dimensions: %d poems have wrong embedding size", #invalid_dimensions))
        print("   First 10 IDs: " .. table.concat({unpack(invalid_dimensions, 1, 10)}, ", "))
    end
    print("")
    print("   Please regenerate embeddings before pre-computing diversity:")
    print("   lua src/similarity-engine.lua -I  (option 1)")
    print("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")
    os.exit(1)
end

print("‚úÖ Data files loaded")
print(string.format("   üìÑ Poems: %d", #poems_data.poems))
print(string.format("   üßÆ Embeddings: %d (dim=%d)", #embeddings_data.embeddings, embedding_dim))
if random_embeddings > 0 then
    print(string.format("   ‚ÑπÔ∏è  %d empty poems have random embeddings (expected)", random_embeddings))
end

-- Build flat embeddings array and poem index mapping
local all_embeddings_flat = {}
local poem_id_to_index = {}
local index_to_poem_id = {}
local valid_poem_ids = {}

for i, poem in ipairs(poems_data.poems) do
    if poem.id and embeddings_data.embeddings[i] and embeddings_data.embeddings[i].embedding then
        -- Store embedding
        for _, val in ipairs(embeddings_data.embeddings[i].embedding) do
            table.insert(all_embeddings_flat, val)
        end

        poem_id_to_index[poem.id] = i
        index_to_poem_id[i] = poem.id
        table.insert(valid_poem_ids, poem.id)
    end
end

local num_poems = #valid_poem_ids
table.sort(valid_poem_ids)

-- Limit in test mode
if TEST_MODE then
    local limited_ids = {}
    for i = 1, math.min(MAX_TEST_SEQUENCES, #valid_poem_ids) do
        table.insert(limited_ids, valid_poem_ids[i])
    end
    valid_poem_ids = limited_ids
end

print(string.format("   üî¢ Sequences to compute: %d", #valid_poem_ids))

-- Create temp directory for sequence files
local temp_dir = DIR .. "/assets/embeddings/EmbeddingGemma_latest/diversity_temp"
os.execute("mkdir -p " .. temp_dir)

-- Convert embeddings to effil-shareable format (single-threaded, ~30-60 seconds)
print("\nüì¶ Converting embeddings for parallel processing (single-threaded)...")
io.write(string.format("   Converting %d values to effil table... ", #all_embeddings_flat))
io.flush()
local shared_embeddings = effil.table(all_embeddings_flat)
print("done")
print("   ‚úÖ Ready for multi-threaded computation")

-- Estimate time
local est_time_per_seq = 25  -- seconds
local est_total_time = (#valid_poem_ids * est_time_per_seq) / NUM_THREADS
local est_hours = est_total_time / 3600
print(string.format("\n‚è±Ô∏è  Estimated time: %.1f hours (%.0f seconds per sequence)", est_hours, est_time_per_seq))
print(string.format("   With %d-second sleep every %d sequences", SLEEP_BETWEEN_BATCHES, NUM_THREADS))

-- Pre-computation (MULTI-THREADED phase)
print("\nüöÄ Starting diversity sequence pre-computation (MULTI-THREADED)...")
print(string.format("   Using %d threads with %d-second cooling between batches", NUM_THREADS, SLEEP_BETWEEN_BATCHES))
print("   Note: This is a one-time cost. Results will be cached.")
local start_time = os.time()

local completed = 0
local failed = 0
local batch_num = 0

-- Process in batches with thermal management
local batch_start = 1
while batch_start <= #valid_poem_ids do
    batch_num = batch_num + 1
    local batch_end = math.min(batch_start + NUM_THREADS - 1, #valid_poem_ids)
    local batch_size = batch_end - batch_start + 1

    print(string.format("\n   Batch %d: sequences %d-%d", batch_num, batch_start, batch_end))

    -- Start all threads in this batch simultaneously
    local threads = {}
    local batch_start_time = os.time()

    for i = batch_start, batch_end do
        local poem_id = valid_poem_ids[i]
        local poem_index = poem_id_to_index[poem_id]

        threads[i - batch_start + 1] = effil.thread(diversity_sequence_worker)(
            poem_id,
            poem_index,
            shared_embeddings,
            embedding_dim,
            num_poems,
            temp_dir
        )
    end

    -- Wait for ALL threads in batch to complete
    for i = 1, batch_size do
        local status = threads[i]:wait()
        local poem_id = valid_poem_ids[batch_start + i - 1]
        local filename = string.format("%s/seq_%d.json", temp_dir, poem_id)
        local f = io.open(filename, "r")
        if f then
            f:close()
            completed = completed + 1
        else
            failed = failed + 1
            print(string.format("      ‚ö†Ô∏è  Failed: poem %d", poem_id))
        end
    end

    local batch_elapsed = os.time() - batch_start_time
    local total_elapsed = os.time() - start_time
    local rate = completed / math.max(total_elapsed, 1)
    local remaining = #valid_poem_ids - completed - failed
    local eta_seconds = remaining / math.max(rate, 0.001)
    local eta_hours = eta_seconds / 3600

    print(string.format("   ‚úÖ Batch complete: %d/%d total (%.2f seq/min, ETA: %.1f hours)",
        completed, #valid_poem_ids, rate * 60, eta_hours))

    -- Thermal management: sleep before next batch (unless this is the last batch)
    if batch_end < #valid_poem_ids then
        print(string.format("   üò¥ Cooling down for %d seconds...", SLEEP_BETWEEN_BATCHES))
        os.execute(string.format("sleep %d", SLEEP_BETWEEN_BATCHES))
    end

    batch_start = batch_end + 1
end

local total_elapsed = os.time() - start_time
print(string.format("\n‚úÖ Pre-computation complete: %d sequences in %d seconds (%.1f hours)",
    completed, total_elapsed, total_elapsed / 3600))

if failed > 0 then
    print(string.format("‚ö†Ô∏è  %d sequences failed", failed))
end

-- Collect all sequences into final cache file
print("\nüì¶ Collecting sequences into cache file...")
local cache = {
    metadata = {
        generated_at = os.date("%Y-%m-%d %H:%M:%S"),
        total_sequences = completed,
        algorithm = "centroid-based-diversity",
        embedding_dimension = embedding_dim,
        source_embeddings = "EmbeddingGemma_latest"
    },
    sequences = {}
}

for _, poem_id in ipairs(valid_poem_ids) do
    local filename = string.format("%s/seq_%d.json", temp_dir, poem_id)
    local f = io.open(filename, "r")
    if f then
        local content = f:read("*a")
        f:close()

        -- Parse sequence (array of indices) and convert to poem IDs
        local sequence_indices = dkjson.decode(content)
        if sequence_indices then
            local sequence_ids = {}
            for _, idx in ipairs(sequence_indices) do
                local pid = index_to_poem_id[idx]
                if pid then
                    table.insert(sequence_ids, pid)
                end
            end
            cache.sequences[tostring(poem_id)] = sequence_ids
        end
    end
end

-- Write cache file
local cache_file = DIR .. "/assets/embeddings/EmbeddingGemma_latest/diversity_cache.json"
local cache_json = dkjson.encode(cache, {indent = false})
local f = io.open(cache_file, "w")
if f then
    f:write(cache_json)
    f:close()
    print(string.format("‚úÖ Cache file written: %s", relative_path(cache_file)))

    -- Get file size
    local handle = io.popen(string.format("ls -lh '%s' | awk '{print $5}'", cache_file))
    if handle then
        local size = handle:read("*a"):gsub("%s+", "")
        handle:close()
        print(string.format("   Size: %s", size))
    end
else
    print("‚ùå Error: Could not write cache file")
end

-- Cleanup temp files
print("\nüßπ Cleaning up temp files...")
os.execute(string.format("rm -rf '%s'", temp_dir))

print("\n" .. string.rep("=", 61))
print("Pre-computation Summary")
print(string.rep("=", 61))
print(string.format("   Sequences computed: %d", completed))
print(string.format("   Failed: %d", failed))
print(string.format("   Total time: %d seconds (%.1f hours)", total_elapsed, total_elapsed / 3600))
print(string.format("   Cache file: %s", relative_path(cache_file)))
print("\nYou can now run generate-html-parallel which will use the cached sequences.")
-- }}}
