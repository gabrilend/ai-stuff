#!/usr/bin/env luajit

-- Pre-computes diversity sequences for all poems and caches to disk
-- Uses centroid-based O(n^2) algorithm with thermal management (sleep between batches)
-- Output: assets/embeddings/embeddinggemma_latest/diversity_cache.json
--
-- OPTIMIZATIONS (Issue 9-003):
-- 1. Incremental running sum: O(768) per iteration instead of O(N √ó 768)
--    - Keeps a running sum instead of recalculating centroid from scratch
--    - ~4,000√ó faster centroid maintenance
--
-- 2. No division needed: cosine distance is scale-invariant
--    - cos(kA, B) = cos(A, B) for any scalar k
--    - Running sum gives same ranking as running average
--
-- 3. RAM-only storage: no temp files, sequences stored in memory
--    - Reduces disk I/O and SSD wear
--    - Results returned directly from worker threads via effil

-- {{{ local function setup_dir_path
local function setup_dir_path(provided_dir)
    if provided_dir then
        return provided_dir
    end
    return "/mnt/mtwo/programming/ai-stuff/neocities-modernization"
end
-- }}}

local DIR = setup_dir_path(arg and arg[1])

-- Add effil library path
package.cpath = "/home/ritz/programming/ai-stuff/libs/lua/effil-jit/build/?.so;" .. package.cpath
package.path = DIR .. "/libs/?.lua;" .. DIR .. "/src/?.lua;" .. package.path

local effil = require("effil")
local utils = require("utils")
local dkjson = require("dkjson")

-- {{{ local function relative_path
local function relative_path(absolute_path)
    if absolute_path:sub(1, #DIR) == DIR then
        local rel = absolute_path:sub(#DIR + 1)
        if rel:sub(1, 1) == "/" then rel = rel:sub(2) end
        return "./" .. rel
    end
    return absolute_path
end
-- }}}

-- Configuration
local NUM_THREADS = tonumber(arg and arg[2]) or 4
local SLEEP_BETWEEN_BATCHES = tonumber(arg and arg[3]) or 5
local TEST_MODE = arg and arg[4] == "--test"
local MAX_TEST_SEQUENCES = 10

-- {{{ Worker function for diversity sequence computation
-- Computes full diversity sequence using INCREMENTAL CENTROID optimization
-- Returns sequence directly (no temp files) - RAM-only storage
--
-- OPTIMIZATION NOTES (Issue 9-003):
-- 1. Incremental running sum instead of recalculating centroid from scratch
--    - Old: O(N √ó 768) per iteration where N grows to 7,792
--    - New: O(768) per iteration (constant time)
--    - Speedup: ~4,000√ó for centroid maintenance
--
-- 2. No division needed: cosine distance is scale-invariant
--    - cos(kA, B) = cos(A, B) for any scalar k
--    - Running sum gives same ranking as running average
--
-- 3. RAM-only: return sequence directly, no temp file I/O
--
-- 4. Per-thread file I/O: each thread reads embeddings independently
--    - Eliminates effil.table conversion bottleneck (30-60 seconds ‚Üí 0)
--    - OS disk cache ensures subsequent reads are fast
--    - Trade-off: ~100MB RAM per thread vs shared access
--
local function diversity_sequence_worker(poem_id, poem_index, embeddings_file, embedding_dim,
                                          num_poems, dkjson_path)
    -- {{{ Load embeddings from file (per-thread I/O)
    -- Each thread reads independently - OS caches the file after first read
    package.path = dkjson_path .. ";" .. package.path
    local dkjson = require("dkjson")

    local f = io.open(embeddings_file, "r")
    if not f then
        return nil, "Could not open embeddings file"
    end
    local content = f:read("*a")
    f:close()

    local embeddings_data = dkjson.decode(content)
    if not embeddings_data or not embeddings_data.embeddings then
        return nil, "Could not parse embeddings JSON"
    end

    -- Build flat embeddings array for fast access
    local all_embeddings_flat = {}
    for i = 1, num_poems do
        if embeddings_data.embeddings[i] and embeddings_data.embeddings[i].embedding then
            for _, val in ipairs(embeddings_data.embeddings[i].embedding) do
                table.insert(all_embeddings_flat, val)
            end
        else
            -- Fill with zeros for missing embeddings (shouldn't happen after validation)
            for _ = 1, embedding_dim do
                table.insert(all_embeddings_flat, 0)
            end
        end
    end
    -- }}}

    -- {{{ Helper: cosine distance between two embeddings
    -- Uses running sum directly (no normalization needed for ranking)
    local function cosine_distance(emb1, emb2)
        local dot_product = 0
        local norm1 = 0
        local norm2 = 0

        for i = 1, #emb1 do
            dot_product = dot_product + (emb1[i] * emb2[i])
            norm1 = norm1 + (emb1[i] * emb1[i])
            norm2 = norm2 + (emb2[i] * emb2[i])
        end

        norm1 = math.sqrt(norm1)
        norm2 = math.sqrt(norm2)

        if norm1 == 0 or norm2 == 0 then
            return 1.0
        end

        return 1.0 - (dot_product / (norm1 * norm2))
    end
    -- }}}

    -- {{{ Helper: extract embedding from flat array
    local function get_embedding(idx)
        local emb_start = (idx - 1) * embedding_dim + 1
        local embedding = {}
        for j = 1, embedding_dim do
            embedding[j] = all_embeddings_flat[emb_start + j - 1]
        end
        return embedding
    end
    -- }}}

    -- Build starting embedding and initialize running sum
    local starting_embedding = get_embedding(poem_index)

    -- OPTIMIZATION: Keep running sum instead of list of embeddings
    -- No division needed - cosine distance is scale-invariant
    local running_sum = {}
    for i = 1, embedding_dim do
        running_sum[i] = starting_embedding[i]
    end

    -- Build list of other poem indices
    local remaining_indices = {}
    for i = 1, num_poems do
        if i ~= poem_index then
            table.insert(remaining_indices, i)
        end
    end

    -- Generate diversity sequence using INCREMENTAL centroid-based selection
    local sequence = {poem_index}  -- Start with this poem's index

    while #remaining_indices > 0 do
        -- Find poem with maximum distance from running_sum (acts as centroid)
        -- No need to divide - cosine is scale-invariant, ranking is preserved
        local max_dist = -1
        local max_idx = -1
        local max_remaining_idx = -1

        for i, idx in ipairs(remaining_indices) do
            local embedding = get_embedding(idx)
            local dist = cosine_distance(running_sum, embedding)
            if dist > max_dist then
                max_dist = dist
                max_idx = idx
                max_remaining_idx = i
            end
        end

        if max_idx > 0 then
            table.insert(sequence, max_idx)

            -- OPTIMIZATION: Update running sum incrementally O(768)
            -- instead of recalculating from all embeddings O(N √ó 768)
            local selected_embedding = get_embedding(max_idx)
            for i = 1, embedding_dim do
                running_sum[i] = running_sum[i] + selected_embedding[i]
            end

            table.remove(remaining_indices, max_remaining_idx)
        else
            break
        end
    end

    -- Return sequence directly (RAM-only, no temp file)
    return sequence
end
-- }}}

-- {{{ Main execution
print("=" .. string.rep("=", 60))
print("Diversity Sequence Pre-computation")
print("=" .. string.rep("=", 60))
print(string.format("Project directory: %s", relative_path(DIR)))
print(string.format("Thread count: %d", NUM_THREADS))
print(string.format("Sleep between batches: %d seconds", SLEEP_BETWEEN_BATCHES))
print(string.format("Test mode: %s", TEST_MODE and "yes (first " .. MAX_TEST_SEQUENCES .. " sequences)" or "no"))

-- Load data files
print("\nüîÑ Loading data files (single-threaded phase)...")
local poems_file = DIR .. "/assets/poems.json"
local embeddings_file = DIR .. "/assets/embeddings/embeddinggemma_latest/embeddings.json"

io.write("   Loading poems.json... ")
io.flush()
local poems_data = utils.read_json_file(poems_file)
print("done")

if not poems_data then
    print("‚ùå Error: Could not load poems.json")
    os.exit(1)
end

-- Embedding validation: full check in production, lightweight in test mode
local embedding_dim = 768  -- Default dimension
local embeddings_data = nil
local missing_embeddings = {}
local random_embeddings = 0

if TEST_MODE then
    -- Lightweight check: just verify file exists and is readable
    print("   Checking embeddings.json (lightweight, test mode)...")
    print("   [DEBUG] Step 1: opening file")
    local f = io.open(embeddings_file, "r")
    print("   [DEBUG] Step 2: file handle = " .. tostring(f))
    if f then
        print("   [DEBUG] Step 3: reading")
        local first_char = f:read(1)
        print("   [DEBUG] Step 4: char = " .. tostring(first_char))
        f:close()
        if first_char == "{" then
            print("   ‚úì File OK")
        else
            print("‚ùå Error: embeddings.json appears malformed")
            os.exit(1)
        end
    else
        print("‚ùå Error: Could not open embeddings.json")
        os.exit(1)
    end
    print("   ‚ö° Skipping full validation in test mode")
else
    -- Full validation in production mode
    io.write("   Loading embeddings.json (62MB, ~1-2 minutes)... ")
    io.flush()
    embeddings_data = utils.read_json_file(embeddings_file)
    print("done")

    if not embeddings_data then
        print("‚ùå Error: Could not load embeddings.json")
        os.exit(1)
    end

    embedding_dim = embeddings_data.metadata and embeddings_data.metadata.embedding_dimension or 768
    local invalid_dimensions = {}

    for i, poem in ipairs(poems_data.poems) do
        if poem.id and poem.content and poem.content ~= "" then
            local emb_entry = embeddings_data.embeddings[i]
            if not emb_entry or not emb_entry.embedding then
                table.insert(missing_embeddings, poem.id)
            elseif type(emb_entry.embedding) ~= "table" or #emb_entry.embedding ~= embedding_dim then
                table.insert(invalid_dimensions, poem.id)
            elseif emb_entry.is_random then
                random_embeddings = random_embeddings + 1
            end
        end
    end

    if #missing_embeddings > 0 or #invalid_dimensions > 0 then
        print("\n‚ùå EMBEDDING VALIDATION FAILED")
        print("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")
        if #missing_embeddings > 0 then
            print(string.format("   Missing embeddings: %d poems with content have no embedding", #missing_embeddings))
            print("   First 10 IDs: " .. table.concat({unpack(missing_embeddings, 1, 10)}, ", "))
        end
        if #invalid_dimensions > 0 then
            print(string.format("   Invalid dimensions: %d poems have wrong embedding size", #invalid_dimensions))
            print("   First 10 IDs: " .. table.concat({unpack(invalid_dimensions, 1, 10)}, ", "))
        end
        print("")
        print("   Please regenerate embeddings before pre-computing diversity:")
        print("   lua src/similarity-engine.lua -I  (option 1)")
        print("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")
        os.exit(1)
    end

    print("‚úÖ Data files loaded")
    print(string.format("   üìÑ Poems: %d", #poems_data.poems))
    print(string.format("   üßÆ Embeddings: %d (dim=%d)", #embeddings_data.embeddings, embedding_dim))
    if random_embeddings > 0 then
        print(string.format("   ‚ÑπÔ∏è  %d empty poems have random embeddings (expected)", random_embeddings))
    end
end

-- Build poem index mappings (embeddings loaded per-thread now, not here)
local poem_id_to_index = {}
local index_to_poem_id = {}
local valid_poem_ids = {}

for i, poem in ipairs(poems_data.poems) do
    if poem.id then
        if TEST_MODE then
            -- In test mode, assume all poems are valid (workers will handle errors)
            poem_id_to_index[poem.id] = i
            index_to_poem_id[i] = poem.id
            table.insert(valid_poem_ids, poem.id)
        elseif embeddings_data and embeddings_data.embeddings[i] and embeddings_data.embeddings[i].embedding then
            -- In production mode, only include poems with valid embeddings
            poem_id_to_index[poem.id] = i
            index_to_poem_id[i] = poem.id
            table.insert(valid_poem_ids, poem.id)
        end
    end
end

local num_poems = #poems_data.poems  -- Total poems (for worker indexing)
table.sort(valid_poem_ids)

-- Limit in test mode
if TEST_MODE then
    local limited_ids = {}
    for i = 1, math.min(MAX_TEST_SEQUENCES, #valid_poem_ids) do
        table.insert(limited_ids, valid_poem_ids[i])
    end
    valid_poem_ids = limited_ids
end

print(string.format("   üî¢ Sequences to compute: %d", #valid_poem_ids))

-- RAM-only storage: no temp files needed (Issue 9-003 optimization)
-- Sequences are returned directly from worker threads and stored in memory
local computed_sequences = {}  -- poem_id -> sequence (array of indices)

-- Per-thread I/O: no effil.table conversion needed! (Issue 9-003 optimization)
-- Each thread reads the embeddings file independently - OS caches it after first read
print("\nüì¶ Per-thread I/O mode enabled (no effil.table conversion)")
print("   Each thread will read embeddings independently (~2-3s per thread, cached)")
print("   ‚úÖ Ready for multi-threaded computation")

-- Path to dkjson library for workers
local dkjson_path = DIR .. "/libs/?.lua"

-- Estimate time (with all optimizations from Issue 9-003)
-- Old: ~25s per sequence, New: ~3-8s per sequence expected
local est_time_per_seq = 5  -- seconds (optimized estimate)
local est_total_time = (#valid_poem_ids * est_time_per_seq) / NUM_THREADS
local est_hours = est_total_time / 3600
print(string.format("\n‚è±Ô∏è  Estimated time: %.1f hours (%.0f seconds per sequence)", est_hours, est_time_per_seq))
print(string.format("   With %d-second sleep every %d sequences", SLEEP_BETWEEN_BATCHES, NUM_THREADS))
print("   ‚ö° Optimizations: incremental centroid + per-thread I/O (Issue 9-003)")

-- Pre-computation (MULTI-THREADED phase)
print("\nüöÄ Starting diversity sequence pre-computation (MULTI-THREADED)...")
print(string.format("   Using %d threads with %d-second cooling between batches", NUM_THREADS, SLEEP_BETWEEN_BATCHES))
print("   Note: This is a one-time cost. Results will be cached.")
local start_time = os.time()

local completed = 0
local failed = 0
local batch_num = 0

-- Process in batches with thermal management
local batch_start = 1
while batch_start <= #valid_poem_ids do
    batch_num = batch_num + 1
    local batch_end = math.min(batch_start + NUM_THREADS - 1, #valid_poem_ids)
    local batch_size = batch_end - batch_start + 1

    print(string.format("\n   Batch %d: sequences %d-%d", batch_num, batch_start, batch_end))

    -- Start all threads in this batch simultaneously
    local threads = {}
    local thread_poem_ids = {}  -- Track which poem ID each thread is processing
    local batch_start_time = os.time()

    for i = batch_start, batch_end do
        local poem_id = valid_poem_ids[i]
        local poem_index = poem_id_to_index[poem_id]
        local thread_idx = i - batch_start + 1

        thread_poem_ids[thread_idx] = poem_id
        threads[thread_idx] = effil.thread(diversity_sequence_worker)(
            poem_id,
            poem_index,
            embeddings_file,  -- Each thread reads independently (OS caches)
            embedding_dim,
            num_poems,
            dkjson_path
        )
    end

    -- Wait for ALL threads in batch to complete and collect results (RAM-only)
    for i = 1, batch_size do
        local status, sequence = threads[i]:get()  -- Get return value directly
        local poem_id = thread_poem_ids[i]

        if status == "completed" and sequence and #sequence > 0 then
            computed_sequences[poem_id] = sequence  -- Store in RAM
            completed = completed + 1
        else
            failed = failed + 1
            print(string.format("      ‚ö†Ô∏è  Failed: poem %d (status: %s)", poem_id, tostring(status)))
        end
    end

    local batch_elapsed = os.time() - batch_start_time
    local total_elapsed = os.time() - start_time
    local rate = completed / math.max(total_elapsed, 1)
    local remaining = #valid_poem_ids - completed - failed
    local eta_seconds = remaining / math.max(rate, 0.001)
    local eta_hours = eta_seconds / 3600

    print(string.format("   ‚úÖ Batch complete: %d/%d total (%.2f seq/min, ETA: %.1f hours)",
        completed, #valid_poem_ids, rate * 60, eta_hours))

    -- Thermal management: sleep before next batch (unless this is the last batch)
    if batch_end < #valid_poem_ids then
        print(string.format("   üò¥ Cooling down for %d seconds...", SLEEP_BETWEEN_BATCHES))
        os.execute(string.format("sleep %d", SLEEP_BETWEEN_BATCHES))
    end

    batch_start = batch_end + 1
end

local total_elapsed = os.time() - start_time
print(string.format("\n‚úÖ Pre-computation complete: %d sequences in %d seconds (%.1f hours)",
    completed, total_elapsed, total_elapsed / 3600))

if failed > 0 then
    print(string.format("‚ö†Ô∏è  %d sequences failed", failed))
end

-- Collect all sequences into final cache file (from RAM, no temp files)
print("\nüì¶ Building cache from RAM-stored sequences...")
local cache = {
    metadata = {
        generated_at = os.date("%Y-%m-%d %H:%M:%S"),
        total_sequences = completed,
        algorithm = "centroid-based-diversity-incremental",  -- Issue 9-003 optimized
        embedding_dimension = embedding_dim,
        source_embeddings = "embeddinggemma_latest",
        optimization_notes = "Incremental running sum, no division, RAM-only storage"
    },
    sequences = {}
}

-- Convert sequences from indices to poem IDs (already in RAM)
for poem_id, sequence_indices in pairs(computed_sequences) do
    local sequence_ids = {}
    for _, idx in ipairs(sequence_indices) do
        local pid = index_to_poem_id[idx]
        if pid then
            table.insert(sequence_ids, pid)
        end
    end
    cache.sequences[tostring(poem_id)] = sequence_ids
end

print(string.format("   Converted %d sequences to poem IDs", completed))

-- Write cache file
local cache_file = DIR .. "/assets/embeddings/embeddinggemma_latest/diversity_cache.json"
local cache_json = dkjson.encode(cache, {indent = false})
local f = io.open(cache_file, "w")
if f then
    f:write(cache_json)
    f:close()
    print(string.format("‚úÖ Cache file written: %s", relative_path(cache_file)))

    -- Get file size
    local handle = io.popen(string.format("ls -lh '%s' | awk '{print $5}'", cache_file))
    if handle then
        local size = handle:read("*a"):gsub("%s+", "")
        handle:close()
        print(string.format("   Size: %s", size))
    end
else
    print("‚ùå Error: Could not write cache file")
end

-- No temp file cleanup needed - RAM-only storage (Issue 9-003)

print("\n" .. string.rep("=", 61))
print("Pre-computation Summary")
print(string.rep("=", 61))
print(string.format("   Sequences computed: %d", completed))
print(string.format("   Failed: %d", failed))
print(string.format("   Total time: %d seconds (%.1f hours)", total_elapsed, total_elapsed / 3600))
print(string.format("   Cache file: %s", relative_path(cache_file)))
print("\nYou can now run generate-html-parallel which will use the cached sequences.")
-- }}}
